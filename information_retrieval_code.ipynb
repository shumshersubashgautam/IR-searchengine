{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwuLpYOU+v2oo8OpzgBkH4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shumshersubashgautam/IR-searchengine/blob/main/information_retrieval_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "WVxKVoGcox9N",
        "outputId": "1e4c994f-70ef-4544-a45c-2ed858df4e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "696\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-dccd7901af03>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#Seperate name, url, date, author in different file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mpubName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mpubURL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pub_url\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpubCUAuthor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cu_author\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'name'"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk #NLTK for natural language processing tasks\n",
        "from nltk.corpus import stopwords # list of stop word\n",
        "from nltk.tokenize import word_tokenize # To tokenize each word\n",
        "from nltk.stem import PorterStemmer # For specific rules to transform words to their stems\n",
        "import ujson\n",
        "\n",
        "# Preprosessing data before indexing\n",
        "with open('result.json', 'r') as doc: scraper_results=doc.read()\n",
        "\n",
        "# Initialize empty lists to store publication name, URL, author, and date\n",
        "\n",
        "pubName = []\n",
        "pubURL = []\n",
        "pubCUAuthor = []\n",
        "pubDate = []\n",
        "\n",
        "# Load the scraped results using ujson\n",
        "data_dict = ujson.loads(scraper_results)\n",
        "\n",
        "# Get the length of the data_dict (number of publications)\n",
        "array_length = len(data_dict)\n",
        "# Print the number of publications\n",
        "print(array_length)\n",
        "\n",
        "#Seperate name, url, date, author in different file\n",
        "for item in data_dict:\n",
        "    pubName.append(item[\"name\"])\n",
        "    pubURL.append(item[\"pub_url\"])\n",
        "    pubCUAuthor.append(item[\"cu_author\"])\n",
        "    pubDate.append(item[\"date\"])\n",
        "with open('pub_name.json', 'w') as f:ujson.dump(pubName, f)\n",
        "with open('pub_url.json', 'w') as f:ujson.dump(pubURL, f)\n",
        "with open('pub_cu_author.json', 'w') as f:ujson.dump(pubCUAuthor, f)\n",
        "with open('pub_date.json', 'w') as f: ujson.dump(pubDate, f)\n",
        "\n",
        "\n",
        "#Open a file with publication names in read mode\n",
        "with open('pub_name.json','r') as f:publication=f.read()\n",
        "\n",
        "#Load JSON File\n",
        "pubName = ujson.loads(publication)\n",
        "\n",
        "#Downloading libraries to use its methods\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Predefined stopwords in nltk are used\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "pub_list_first_stem = []\n",
        "pub_list = []\n",
        "pub_list_wo_sc = []\n",
        "print(len(pubName))\n",
        "\n",
        "for file in pubName:\n",
        "    #Splitting strings to tokens(words)\n",
        "    words = word_tokenize(file)\n",
        "    stem_word = \"\"\n",
        "    for i in words:\n",
        "        if i.lower() not in stop_words:\n",
        "            stem_word += stemmer.stem(i) + \" \"\n",
        "    pub_list_first_stem.append(stem_word)\n",
        "    pub_list.append(file)\n",
        "\n",
        "#Removing all below characters\n",
        "special_characters = '''!()-—[]{};:'\"\\, <>./?@#$%^&*_~0123456789+=’‘'''\n",
        "for file in pub_list:\n",
        "    word_wo_sc = \"\"\n",
        "    if len(file.split()) ==1 : pub_list_wo_sc.append(file)\n",
        "    else:\n",
        "        for a in file:\n",
        "            if a in special_characters:\n",
        "                word_wo_sc += ' '\n",
        "            else:\n",
        "                word_wo_sc += a\n",
        "        #print(word_wo_sc)\n",
        "        pub_list_wo_sc.append(word_wo_sc)\n",
        "\n",
        "#Stemming Process\n",
        "pub_list_stem_wo_sw = []\n",
        "for name in pub_list_wo_sc:\n",
        "    words = word_tokenize(name)\n",
        "    stem_word = \"\"\n",
        "    for a in words:\n",
        "        if a.lower() not in stop_words:\n",
        "            stem_word += stemmer.stem(a) + ' '\n",
        "    pub_list_stem_wo_sw.append(stem_word.lower())\n",
        "\n",
        "data_dict = {} #Inverted Index holder\n",
        "\n",
        "# Indexing process\n",
        "for a in range(len(pub_list_stem_wo_sw)):\n",
        "    for b in pub_list_stem_wo_sw[a].split():\n",
        "        if b not in data_dict:\n",
        "             data_dict[b] = [a]\n",
        "        else:\n",
        "            data_dict[b].append(a)\n",
        "\n",
        "# printing the lenght\n",
        "print(len(pub_list_wo_sc))\n",
        "print(len(pub_list_stem_wo_sw))\n",
        "print(len(pub_list_first_stem))\n",
        "print(len(pub_list))\n",
        "\n",
        "# with open('publication_list.json', 'w') as f:\n",
        "#     ujson.dump(pub_list, f)\n",
        "\n",
        "with open('publication_list_stemmed.json', 'w') as f:\n",
        "    ujson.dump(pub_list_first_stem, f)\n",
        "\n",
        "with open('publication_indexed_dictionary.json', 'w') as f:\n",
        "    ujson.dump(data_dict, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ujson"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIwJuGq9pb9b",
        "outputId": "15e08451-d896-4029-a3a1-abd1616baab4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ujson\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ujson\n",
            "Successfully installed ujson-5.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/franklinobasy/Web-Scrape-Research-Publication.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80au3ZBSp9vZ",
        "outputId": "ff4ffe16-adfb-4bc2-94a8-af5b43ba11f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Web-Scrape-Research-Publication'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 40 (delta 14), reused 38 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), 53.21 KiB | 2.96 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Web-Scrape-Research-Publication/publication/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XtqS0yspe_X",
        "outputId": "95b52960-3710-4673-84f8-44dde967cd21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Web-Scrape-Research-Publication/publication\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import requests\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class Extract():\n",
        "    '''\n",
        "    This class contains tools for webscraping Coventory University website\n",
        "    for School of Economics, Finance and Accounting Research\n",
        "\n",
        "    '''\n",
        "    __url: str = \"https://pureportal.coventry.ac.uk/en/organisations/centre-for-intelligent-healthcare/publications/\"\n",
        "    __pubs: List[dict] = []\n",
        "    __headers: dict = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"\n",
        "    }\n",
        "    _page_to_parse: int = 14\n",
        "    __has_started: bool = False\n",
        "\n",
        "    def __init__(self, page_to_parse: int = 14):\n",
        "        '''\n",
        "        Argument:\n",
        "            page_to_page: Numnber of pages to parse\n",
        "        '''\n",
        "        self._page_to_parse = page_to_parse\n",
        "\n",
        "    @property\n",
        "    def page_to_parse(self):\n",
        "        return self._page_to_parse\n",
        "\n",
        "    @page_to_parse.setter\n",
        "    def page_to_parse(self, page_to_parse):\n",
        "        if not isinstance(page_to_parse, int):\n",
        "            raise TypeError(f\"argument 'page_to_parse' must be an integer, \\\n",
        "                            {type(page_to_parse)} given\")\n",
        "\n",
        "        self._page_to_parse = page_to_parse\n",
        "\n",
        "    @property\n",
        "    def pubs(self):\n",
        "        if self.__has_started:\n",
        "            return self.__pubs\n",
        "        else:\n",
        "            raise TypeError(\"Web scrape engine has not been executed.\")\n",
        "\n",
        "    @classmethod\n",
        "    def check_author_in_author_links(cls, authors_with_link: List[str], author: str) -> bool:\n",
        "        '''\n",
        "        Checks if an author is represent in list of authors with links\n",
        "\n",
        "        Arguments:\n",
        "            authors_with_links: a list of authors with links\n",
        "            author: Name of author to check\n",
        "\n",
        "        Return:\n",
        "            Bool\n",
        "        '''\n",
        "\n",
        "        for author_ in authors_with_link:\n",
        "            if author == author_:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def webscrape(self) -> None:\n",
        "        '''\n",
        "        webscrape engine\n",
        "        '''\n",
        "        for page in range(self._page_to_parse):\n",
        "            if page != 0:\n",
        "                self.__url = f\"https://pureportal.coventry.ac.uk/en/organisations/centre-for-intelligent-healthcare/publications/?page={page}\"\n",
        "\n",
        "            r = requests.get(self.__url, headers=self.__headers,)\n",
        "            soup = BeautifulSoup(r.content, features=\"lxml\")\n",
        "\n",
        "            for item in soup.find_all(\"li\", class_=\"list-result-item\"):\n",
        "                pub_object = {\n",
        "                    \"publication\": \"\",\n",
        "                    \"publication_link\": \"\",\n",
        "                }\n",
        "\n",
        "                if item.find_all(\"a\", rel=\"ContributionToJournal\"):\n",
        "                    items = item.find_all(\"a\", rel=\"ContributionToJournal\")\n",
        "                elif item.find_all(\"a\", rel=\"ContributionToBookAnthology\"):\n",
        "                    items = item.find_all(\"a\", rel=\"ContributionToBookAnthology\")\n",
        "\n",
        "                for pub in items:\n",
        "                    pub_object[\"publication_link\"] = pub['href']\n",
        "                    pub_object['publication'] = pub.find(\"span\").text\n",
        "\n",
        "                # get authors with link\n",
        "                authors_with_link = []\n",
        "                char = 'a'\n",
        "\n",
        "                for pub in item.find_all(\"a\", class_=\"link person\"):\n",
        "                    link = pub.get(\"href\", None)\n",
        "                    author = pub.find(\"span\").text\n",
        "                    pub_object[f\"author_{char}\"] = author\n",
        "                    pub_object[f\"author_{char}_profile\"] = link\n",
        "\n",
        "                    authors_with_link.append(f\"{author}\")\n",
        "\n",
        "                    char = chr(ord(char) + 1)\n",
        "\n",
        "                # get authors without link\n",
        "                for pub in item.find_all(\"span\", class_=\"\")[1:-1]:\n",
        "                    if not Extract.check_author_in_author_links(authors_with_link, pub.text):\n",
        "                        pub_object[f\"author_{char}\"] = pub.text\n",
        "                        pub_object[f\"author_{char}_profile\"] = None\n",
        "                        char = chr(ord(char) + 1)\n",
        "\n",
        "                self.__pubs.append(pub_object)\n",
        "\n",
        "    def run(self) -> None:\n",
        "        '''\n",
        "        Start Web scraping\n",
        "        '''\n",
        "        self.webscrape()\n",
        "        self.__has_started = True\n",
        "\n",
        "    def result(self) -> None:\n",
        "        if not self.__has_started:\n",
        "            raise TypeError(\"Call start method before calling result method\")\n",
        "\n",
        "        return self.__pubs\n",
        "\n",
        "    def result_tojson(self, path: str) -> None:\n",
        "        '''\n",
        "        Export results to json\n",
        "\n",
        "        Argument:\n",
        "            path: export path\n",
        "\n",
        "        Return:\n",
        "            None\n",
        "        '''\n",
        "        results = self.result()\n",
        "\n",
        "        if path.split('.')[-1] != \"json\":\n",
        "            raise ValueError('File type must be a json')\n",
        "\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(results, f)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # instantiate extraction object\n",
        "    extraction = Extract()\n",
        "\n",
        "    # run the extraction\n",
        "    extraction.run()\n",
        "\n",
        "    # export result\n",
        "    extraction.result_tojson('/content/scrapped_results.json')"
      ],
      "metadata": {
        "id": "jNA8NINWqMn_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk #NLTK for natural language processing tasks\n",
        "from nltk.corpus import stopwords # list of stop word\n",
        "from nltk.tokenize import word_tokenize # To tokenize each word\n",
        "from nltk.stem import PorterStemmer # For specific rules to transform words to their stems\n",
        "import ujson\n",
        "\n",
        "# Preprosessing data before indexing\n",
        "with open('/content/scrapped_results.json', 'r') as doc: scraper_results=doc.read()\n",
        "\n",
        "# Initialize empty lists to store publication name, URL, author, and date\n",
        "\n",
        "pubName = []\n",
        "pubURL = []\n",
        "pubCUAuthor = []\n",
        "pubauthprof = []\n",
        "\n",
        "# Load the scraped results using ujson\n",
        "data_dict = ujson.loads(scraper_results)\n",
        "\n",
        "# Get the length of the data_dict (number of publications)\n",
        "array_length = len(data_dict)\n",
        "# Print the number of publications\n",
        "print(array_length)\n",
        "\n",
        "#Seperate name, url, date, author in different file\n",
        "for item in data_dict:\n",
        "    pubName.append(item[\"publication\"])\n",
        "    pubURL.append(item[\"publication_link\"])\n",
        "    pubCUAuthor.append(item.get(\"author_a\", None))\n",
        "    pubauthprof.append(item.get(\"author_a_profile\", None))\n",
        "with open('pub_name.json', 'w') as f:ujson.dump(pubName, f)\n",
        "with open('pub_url.json', 'w') as f:ujson.dump(pubURL, f)\n",
        "with open('pub_cu_author.json', 'w') as f:ujson.dump(pubCUAuthor, f)\n",
        "with open('pub_date.json', 'w') as f: ujson.dump(pubauthprof, f)\n",
        "\n",
        "\n",
        "#Open a file with publication names in read mode\n",
        "with open('pub_name.json','r') as f:publication=f.read()\n",
        "\n",
        "#Load JSON File\n",
        "pubName = ujson.loads(publication)\n",
        "\n",
        "#Downloading libraries to use its methods\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Predefined stopwords in nltk are used\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "pub_list_first_stem = []\n",
        "pub_list = []\n",
        "pub_list_wo_sc = []\n",
        "print(len(pubName))\n",
        "\n",
        "for file in pubName:\n",
        "    #Splitting strings to tokens(words)\n",
        "    words = word_tokenize(file)\n",
        "    stem_word = \"\"\n",
        "    for i in words:\n",
        "        if i.lower() not in stop_words:\n",
        "            stem_word += stemmer.stem(i) + \" \"\n",
        "    pub_list_first_stem.append(stem_word)\n",
        "    pub_list.append(file)\n",
        "\n",
        "#Removing all below characters\n",
        "special_characters = '''!()-—[]{};:'\"\\, <>./?@#$%^&*_~0123456789+=’‘'''\n",
        "for file in pub_list:\n",
        "    word_wo_sc = \"\"\n",
        "    if len(file.split()) ==1 : pub_list_wo_sc.append(file)\n",
        "    else:\n",
        "        for a in file:\n",
        "            if a in special_characters:\n",
        "                word_wo_sc += ' '\n",
        "            else:\n",
        "                word_wo_sc += a\n",
        "        #print(word_wo_sc)\n",
        "        pub_list_wo_sc.append(word_wo_sc)\n",
        "\n",
        "#Stemming Process\n",
        "pub_list_stem_wo_sw = []\n",
        "for name in pub_list_wo_sc:\n",
        "    words = word_tokenize(name)\n",
        "    stem_word = \"\"\n",
        "    for a in words:\n",
        "        if a.lower() not in stop_words:\n",
        "            stem_word += stemmer.stem(a) + ' '\n",
        "    pub_list_stem_wo_sw.append(stem_word.lower())\n",
        "\n",
        "data_dict = {} #Inverted Index holder\n",
        "\n",
        "# Indexing process\n",
        "for a in range(len(pub_list_stem_wo_sw)):\n",
        "    for b in pub_list_stem_wo_sw[a].split():\n",
        "        if b not in data_dict:\n",
        "             data_dict[b] = [a]\n",
        "        else:\n",
        "            data_dict[b].append(a)\n",
        "\n",
        "# printing the lenght\n",
        "print(len(pub_list_wo_sc))\n",
        "print(len(pub_list_stem_wo_sw))\n",
        "print(len(pub_list_first_stem))\n",
        "print(len(pub_list))\n",
        "\n",
        "# with open('publication_list.json', 'w') as f:\n",
        "#     ujson.dump(pub_list, f)\n",
        "\n",
        "with open('publication_list_stemmed.json', 'w') as f:\n",
        "    ujson.dump(pub_list_first_stem, f)\n",
        "\n",
        "with open('publication_indexed_dictionary.json', 'w') as f:\n",
        "    ujson.dump(data_dict, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TA2CSa1qfyC",
        "outputId": "65cb9f63-2dac-41ad-8272-47bd8ccff24b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "696\n",
            "696\n",
            "696\n",
            "696\n",
            "696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import ujson\n",
        "\n",
        "# Load the JSON file containing scraped results\n",
        "with open('/content/scrapped_results.json', 'r') as doc:\n",
        "    scraper_results = doc.read()\n",
        "\n",
        "# Extract author names from the JSON data\n",
        "authors = []\n",
        "data_dict = ujson.loads(scraper_results)\n",
        "for item in data_dict:\n",
        "    authors.append(item[\"author_a\",  \"author_b\"])\n",
        "\n",
        "# Write the author names to a JSON file\n",
        "with open('author_names.json', 'w') as f:\n",
        "    ujson.dump(authors, f)\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the JSON file containing author names\n",
        "with open('author_names.json', 'r') as f:\n",
        "    author_data = f.read()\n",
        "\n",
        "# Load JSON data\n",
        "authors = ujson.loads(author_data)\n",
        "\n",
        "# Preprocess the author names\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "authors_list_first_stem = []\n",
        "authors_list = []\n",
        "\n",
        "for author in authors:\n",
        "    words = word_tokenize(author)\n",
        "    stem_word = \"\"\n",
        "    for word in words:\n",
        "        if word.lower() not in stop_words:\n",
        "            stem_word += stemmer.stem(word) + \" \"\n",
        "    authors_list_first_stem.append(stem_word)\n",
        "    authors_list.append(author)\n",
        "\n",
        "# Indexing process\n",
        "data_dict = {}\n",
        "for i in range(len(authors_list_first_stem)):\n",
        "    for word in authors_list_first_stem[i].split():\n",
        "        if word not in data_dict:\n",
        "            data_dict[word] = [i]\n",
        "        else:\n",
        "            data_dict[word].append(i)\n",
        "\n",
        "# Write the preprocessed author names and indexed dictionary to JSON files\n",
        "with open('author_list_stemmed.json', 'w') as f:\n",
        "    ujson.dump(authors_list_first_stem, f)\n",
        "\n",
        "with open('author_indexed_dictionary.json', 'w') as f:\n",
        "    ujson.dump(data_dict, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "EZO7xHuAt-b1",
        "outputId": "7908c99e-54ef-49d6-dddd-323c144da6c6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e246e72d5bfc>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mujson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscraper_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mauthors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"author_a\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"author_b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Write the author names to a JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('author_a', 'author_b')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(item.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npY-BTDJ0pVg",
        "outputId": "8d86c083-81b2-42b4-da18-fc83e481c8ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['publication', 'publication_link', 'author_a', 'author_a_profile', 'author_b', 'author_b_profile', 'author_c', 'author_c_profile', 'author_d', 'author_d_profile', 'author_e', 'author_e_profile', 'author_f', 'author_f_profile', 'author_g', 'author_g_profile', 'author_h', 'author_h_profile'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import ujson\n",
        "\n",
        "# Load the JSON file containing scraped results\n",
        "with open('/content/scrapped_results.json', 'r') as doc:\n",
        "    scraper_results = doc.read()\n",
        "\n",
        "# Extract author names from the JSON data\n",
        "authors = []\n",
        "data_dict = ujson.loads(scraper_results)\n",
        "for item in data_dict:\n",
        "    if \"author_a\" in item:\n",
        "        authors.append(item[\"author_a\"])\n",
        "    elif \"author_b\" in item:\n",
        "        authors.append(item[\"author_b\"])\n",
        "    elif \"author_c\" in item:\n",
        "        authors.append(item[\"author_c\"])\n",
        "    elif \"author_d\" in item:\n",
        "        authors.append(item[\"author_d\"])\n",
        "    elif \"author_e\" in item:\n",
        "        authors.append(item[\"author_e\"])\n",
        "    elif \"author_f\" in item:\n",
        "        authors.append(item[\"author_f\"])\n",
        "    elif \"author_g\" in item:\n",
        "        authors.append(item[\"author_g\"])\n",
        "    elif \"author_h\" in item:\n",
        "        authors.append(item[\"author_h\"])\n",
        "\n",
        "# Write the author names to a JSON file\n",
        "with open('author_names.json', 'w') as f:\n",
        "    ujson.dump(authors, f)\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the JSON file containing author names\n",
        "with open('author_names.json', 'r') as f:\n",
        "    author_data = f.read()\n",
        "\n",
        "# Load JSON data\n",
        "authors = ujson.loads(author_data)\n",
        "\n",
        "# Preprocess the author names\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "authors_list_first_stem = []\n",
        "authors_list = []\n",
        "\n",
        "for author in authors:\n",
        "    words = word_tokenize(author)\n",
        "    stem_word = \"\"\n",
        "    for word in words:\n",
        "        if word.lower() not in stop_words:\n",
        "            stem_word += stemmer.stem(word) + \" \"\n",
        "    stripped_stem_word = stem_word.strip() if stem_word else stem_word\n",
        "    authors_list_first_stem.append(stripped_stem_word)\n",
        "    authors_list.append(author)\n",
        "\n",
        "# Indexing process\n",
        "data_dict = {}\n",
        "for i, stemmed_author in enumerate(authors_list_first_stem):\n",
        "    for word in stemmed_author.split():\n",
        "        if word not in data_dict:\n",
        "            data_dict[word] = [i]\n",
        "        else:\n",
        "            data_dict[word].append(i)\n",
        "\n",
        "# Write the preprocessed author names and indexed dictionary to JSON files\n",
        "with open('author_list_stemmed.json', 'w') as f:\n",
        "    ujson.dump(authors_list_first_stem, f)\n",
        "\n",
        "with open('author_indexed_dictionary.json', 'w') as f:\n",
        "    ujson.dump(data_dict, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df3XoL9O1tjh",
        "outputId": "ff8153e4-75c7-45af-cb5d-c8c58dab1001"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import ujson\n",
        "\n",
        "# Load the JSON file containing scraped results\n",
        "with open('/content/scrapped_results.json', 'r') as doc:\n",
        "    scraper_results = doc.read()\n",
        "\n",
        "# Extract author names from the JSON data\n",
        "authors = []\n",
        "data_dict = ujson.loads(scraper_results)\n",
        "for item in data_dict:\n",
        "    for i in range(8):\n",
        "        if f\"author_{chr(ord('a') + i)}\" in item:\n",
        "            authors.append(item[f\"author_{chr(ord('a') + i)}\"])\n",
        "            if f\"author_{chr(ord('a') + i)}_profile\" in item:\n",
        "                authors.append(item[f\"author_{chr(ord('a') + i)}_profile\"])\n",
        "\n",
        "# Write the author names to a JSON file\n",
        "with open('author_names.json', 'w') as f:\n",
        "    ujson.dump(authors, f)\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the JSON file containing author names\n",
        "with open('author_names.json', 'r') as f:\n",
        "    author_data = f.read()\n",
        "\n",
        "# Load JSON data\n",
        "authors = ujson.loads(author_data)\n",
        "\n",
        "# Preprocess the author names\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "authors_list_first_stem = []\n",
        "authors_list = []\n",
        "\n",
        "for author in authors:\n",
        "    words = word_tokenize(author)\n",
        "    stem_word = \"\"\n",
        "    for word in words:\n",
        "        if word.lower() not in stop_words:\n",
        "            stem_word += stemmer.stem(word) + \" \"\n",
        "    authors_list_first_stem.append(stem_word.strip())\n",
        "    authors_list.append(author)\n",
        "\n",
        "# Indexing process\n",
        "data_dict = {}\n",
        "for i, stemmed_author in enumerate(authors_list_first_stem):\n",
        "    for word in stemmed_author.split():\n",
        "        if word not in data_dict:\n",
        "            data_dict[word] = [i]\n",
        "        else:\n",
        "            data_dict[word].append(i)\n",
        "\n",
        "# Write the preprocessed author names and indexed dictionary to JSON files\n",
        "with open('author_list_stemmed.json', 'w') as f:\n",
        "    ujson.dump(authors_list_first_stem, f)\n",
        "\n",
        "with open('author_indexed_dictionary.json', 'w') as f:\n",
        "    ujson.dump(data_dict, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "f8e8e-aQ-bxe",
        "outputId": "1c47c7e6-e29c-4ece-9796-0743877394aa"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-8c4b4f0ffca7>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mstem_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \"\"\"\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \"\"\"\n\u001b[0;32m-> 1341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_whitespace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \"\"\"\n\u001b[0;32m-> 1341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_whitespace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \"\"\"\n\u001b[1;32m   1458\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0mprevious_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mprevious_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m             \u001b[0;31m# Get the slice of the previous word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import ujson\n",
        "\n",
        "# Load the JSON file containing scraped results\n",
        "with open('/content/scrapped_results.json', 'r') as doc:\n",
        "    scraper_results = doc.read()\n",
        "\n",
        "# Extract author names from the JSON data\n",
        "authors = []\n",
        "data_dict = ujson.loads(scraper_results)\n",
        "for item in data_dict:\n",
        "    for i in range(8):\n",
        "        if f\"author_{chr(ord('a') + i)}\" in item:\n",
        "            authors.append(item[f\"author_{chr(ord('a') + i)}\"])\n",
        "            if f\"author_{chr(ord('a') + i)}_profile\" in item:\n",
        "                authors.append(item[f\"author_{chr(ord('a') + i)}_profile\"])\n",
        "\n",
        "# Write the author names to a JSON file\n",
        "with open('author_names.json', 'w') as f:\n",
        "    ujson.dump(authors, f)\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the JSON file containing author names\n",
        "with open('author_names.json', 'r') as f:\n",
        "    author_data = f.read()\n",
        "\n",
        "# Load JSON data\n",
        "authors = ujson.loads(author_data)\n",
        "\n",
        "# Preprocess the author names\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "authors_list_first_stem = []\n",
        "authors_list = []\n",
        "\n",
        "for author in authors:\n",
        "    words = word_tokenize(author)\n",
        "    stem_word = \"\"\n",
        "    for word in words:\n",
        "        if word.lower() not in stop_words:\n",
        "            stem_word += stemmer.stem(word) + \" \"\n",
        "    stripped_stem_word = stem_word.strip() if stem_word else stem_word\n",
        "    authors_list_first_stem.append(stripped_stem_word)\n",
        "    authors_list.append(author)\n",
        "\n",
        "# Indexing process\n",
        "data_dict = {}\n",
        "for i, stemmed_author in enumerate(authors_list_first_stem):\n",
        "    for word in stemmed_author.split():\n",
        "        if word not in data_dict:\n",
        "            data_dict[word] = [i]\n",
        "        else:\n",
        "            data_dict[word].append(i)\n",
        "\n",
        "# Write the preprocessed author names and indexed dictionary to JSON files\n",
        "with open('author_list_stemmed.json', 'w') as f:\n",
        "    ujson.dump(authors_list_first_stem, f)\n",
        "\n",
        "with open('author_indexed_dictionary.json', 'w') as f:\n",
        "    ujson.dump(data_dict, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "3ByaQfR--70g",
        "outputId": "540872ed-5df7-4981-c116-84110fce23e5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3db26a26ce5d>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mstem_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \"\"\"\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \"\"\"\n\u001b[0;32m-> 1341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_whitespace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \"\"\"\n\u001b[0;32m-> 1341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_whitespace_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \"\"\"\n\u001b[1;32m   1458\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1459\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1460\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0mprevious_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mprevious_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m             \u001b[0;31m# Get the slice of the previous word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_results(output_data, search_type):\n",
        "    aa = 0\n",
        "    rank_sorting = sorted(output_data.items(), key=lambda z: z[1], reverse=True)\n",
        "\n",
        "    # Show the total number of research results\n",
        "    st.info(f\"Showing results for: {len(rank_sorting)}\")\n",
        "\n",
        "    # Show the cards\n",
        "    N_cards_per_row = 3\n",
        "    for n_row, (id_val, ranking) in enumerate(rank_sorting):\n",
        "        i = n_row % N_cards_per_row\n",
        "        if i == 0:\n",
        "            st.write(\"---\")\n",
        "            cols = st.columns(N_cards_per_row, gap=\"large\")\n",
        "        # Draw the card\n",
        "        with cols[n_row % N_cards_per_row]:\n",
        "            if search_type == \"Publications\":\n",
        "                pub_date_val = pub_date[id_val].strip() if pub_date[id_val] is not None else \"\"\n",
        "                st.caption(pub_date_val)\n",
        "                st.markdown(f\"**{pub_cu_author[id_val].strip()}**\")\n",
        "                st.markdown(f\"*{pub_name[id_val].strip()}*\")\n",
        "                st.markdown(f\"**{pub_url[id_val]}**\")\n",
        "            elif search_type == \"Authors\":\n",
        "                pub_date_val = pub_date[id_val].strip() if pub_date[id_val] is not None else \"\"\n",
        "                st.caption(pub_date_val)\n",
        "                st.markdown(f\"**{author_name[id_val].strip()}**\")\n",
        "                st.markdown(f\"*{pub_name[id_val].strip()}*\")\n",
        "                st.markdown(f\"**{pub_url[id_val]}**\")\n",
        "                st.markdown(f\"Ranking: {ranking[0]:.2f}\")\n",
        "\n",
        "        aa += 1\n",
        "\n",
        "    if aa == 0:\n",
        "        st.info(\"No results found. Please try again.\")\n",
        "    else:\n",
        "        st.info(f\"Results shown for: {aa}\")\n"
      ],
      "metadata": {
        "id": "NN4i-I62BHIj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}